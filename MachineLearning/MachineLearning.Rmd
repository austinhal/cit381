---
title: "Learning Machine Learning"
author: "Austin Halvorsen"
date: "`r format(Sys.time(), '%B %d, %Y')`"
assets:
  css:
    - "https://fonts.googleapis.com/css?family=Merriweather|Roboto+Slab"
output: 
  html_document:
    theme: flatly
    highlight: tango
    code_folding: show
---

<style>
h4 {
  color: indianred;
  font-weigh: bold;
}

p  {
  font-family: "Roboto Slab"
}
</style>

## Research Project: Deliverable {.tabset .tabset-fade .tabset-pills}

This project is a self research project for Business Intelligence and Analytics at Brigham Young University - Idaho. The focus of this project has three parts:

1. Introduce Machine Learning and what it is
2. Give an example of machine learning on a sample dataset
3. Summarize my findings and share major learning points

Follow through the project using the tabs below. Also, feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/ahalvo/) and follow me on [GitHub](http://www.github.com/austinhal)

### PROJECT INTRODUCTION

#### Machine Learning

So what is Machine Learning exactly? Machine learning is not a brand new concept or idea. And further more, it's been around for quite some time. The first uses of the theology of machine learning can be traced all the way back to work of [Thomas Bayes](http://www-history.mcs.st-andrews.ac.uk/Biographies/Bayes.html) in the late 1700's who can be credited with much of the way that we view statistics and models in the modern era. While his thinking was ahead of his time, he laid groundwork for how we would one day implement complex machine learning models. 

Machine learning is really an arm of artificial intelligence and statistics. Its definition can be simply boiled down to this: "Machine Learning is the field of study that gives computers the capability to learn without being explicitly programmed." [(Geek for Geeks)](https://www.geeksforgeeks.org/machine-learning/). Since the rise of access to computers and the increase of performance of those machines, machine learning has been *racing* ahead in industries and academia at an amazing rate. Over the last century, complex problems like medical diagnosis and detecting threats at airports have been aided with machine learning techniques and technologies.

To break down a little more of what machine learning is, it helps to understand a typical output and how you are telling the computer to behave when faced with data. I'll go over a brief outline here, but there are a myriad of resources available online if you want to dig deeper. The two main schools of thought are the following:

1. Supervised Learning
2. Unsupervised Learning

These two methods can have several underlying algorithms that will produce the results, but the principles remain the same in either method. In **Supervised Learning**, the computer is given a dataset and then told what the answer is based on the data. For example, if you wanted to see if someone was pregnant, similar to our textbook example, you would feed the computer a set of variables and a field that says the woman is pregnant or not. The computer will *learn* what variables are important and when it see's a new dataset, it will begin to predict which rows of data represent a pregnant woman or not. Cool!

In **Unsupervised Learing**, you are taking a dataset and *not* telling the computer what the desired output is. You let the computer group, or cluster, similarties together and it will determine crucial values and then try to predict an output. This could be used in medical diagnosis where you aren't quite sure what variables are causing a certain disease. The computer can run several versions of the model and give places to start looking for causes to certain diseases. 

There are several statisical approaches to how machine learning can be implemented. Many of which have been reviewed in this course in fact. 

#### So what?

So why has machine learning become so big over the last few years and why should I care? To the average individual, they may never see the influence that machine learning has in their daily lives. Largely due to the fact that machine learning hasn't been able to be used in the scale that it has until computers have had the capability to process that amount of data necessary to make it a useable technology. With computers becoming increasingly capable of performing complex computations quicker and more efficiently than ever before, the boom of machine learning has really taken off completely right by it's side. 

As the world grows, we are faced with complex challenges and questions that require a very precise answer. Often in the form of energy demands and food supply. With the rapid development of countries and urban areas, conventional methods are no longer sufficient enough to tackle such problems and methods, that of Machine Learning, are required to come to an answer.

Moreover, as we make advances in the medical field, machine learning can be used to give more accurate diagnosis of diseases and therefore better treatment of patients. This technology can be used to help find cures for diseases like cancer and AIDS. 

On the flip side, there are some darker aspects of machine learning that people have raised some flag about as well though. Most of them revolve around privacy issues. With these technologies, when you step into public areas, stores are recording customer habits to predict what makes them purchase a product. Street camera's track cars and pedestrians to see if there are accidents happenening or situations that require further action. This information is stored in a database and raises privacy concerns. How safe is this data? Is it regulated to who has access to it? What if this data gets leaked? There are certainly risks to having this information, but do the rewards outweigh the risks? I think so.

#### Application

Now that we have a basic understanding of what machine learning is, I want to do a simple walkthrough of how this would look using a real world dataset. Click on the 'ML EXAMPLE' tab to look at an example of how machine learning would be accomplished in R. 

### ML EXAMPLE

For this example, I want demonstrate all the code that goes into a simple machine learning problem. Machine Learning is not as complex as you might think, and I want to show that through this example of [logistic regression](https://www.statisticssolutions.com/what-is-logistic-regression/), which is a form of [classification](https://en.wikipedia.org/wiki/Statistical_classification). In the document below, I will walk through the three main steps of many machine learning problems. Those steps include:

1. Data Cleaning
2. Data Exploration
3. Creating your model

In this example, we will be trying to predict the salary of people based on a dataset of salaries. 

To start, we need to read in a dataset. For this exmaple, I am going to use a salaries dataset. Before I load that into R, I need to load all related libraries that I will be using throughout this document.

```{r load_library, include=TRUE, warning=FALSE, message=FALSE}
# Load Libraries
library(tidyverse)  # For data manipulation
library(Amelia)     # For finding missing values
library(ggplot2)    # For visualizations
library(caTools)    # For Machine Learning
```

Now that those are running, I will load my dataset.

```{r load_data, include=TRUE}
# Loading Datasets
sal <- read.csv("adult_sal.csv")
```

For the reminder of this example, I will not be writing out each minor step, but instead will be leaving comments in the code. If you are unsure of what something does, google it or leave a comment in the discussion board.

#### Data Clean

```{r}
# Here, I need to remove some columns that won't be used in our analysis
sal <- select(sal, -X)

# Now, I want to clean some of my categorical columns for my logistic regression. In this case, the type_employer column, since this is a predictor column.
sal <- sal %>% 
  mutate(type_employer = case_when(
    type_employer %in% c("Without-pay", "Never-worked") ~ "Unemployed",
    type_employer %in% c("Self-emp-inc", "Self-emp-not-inc") ~ "Self-emp",
    type_employer %in% c("Local-gov", "State-gov") ~ "SL-gov",
    TRUE ~ as.character(type_employer)
  ))

# Combining Marital Status
sal <- sal %>% 
  mutate(marital = case_when(
    marital %in% c("Separated", "Divorced", "Widowed") ~ "Not-married",
    marital %in% c("Never-married") ~ "Never-married",
    marital %in% c("Married-AF-spouse", "Married-civ-spouse", "Married-spouse-absent") ~ "Married",
    TRUE ~ as.character(marital)
  ))

# Combining County into new column region
sal <- sal %>% 
  mutate(region = case_when(
    country %in% c("China", "Hong", "India", "Iran", "Cambodia", "Japan", "Laos", "Philippines", "Vietnam", "Taiwan", "Thailand") ~ "Asia",
    country %in% c("Canada", "United-States", "Puerto-Rico") ~ "North America",
    country %in% c("England", "France", "Germany", "Greece", "Holand-Netherlands", "Hungary", "Ireland", "Italy", "Poland", "Portugal", "Scotland", "Yugoslavia") ~ "Europe",
    country %in% c("Columbia", "Cuba", "Dominican-Republic", "Ecuador", "El-Salvador", "Guatemala", "Haiti", "Honduras", "Mexico", "Nicaragua", "Outlying-US(Guam-USVI-etc)", "Peru", "Jamaica", "Trinadad&Tobago") ~ "Latin-South America",
    country %in% c("South", "?") ~ "Other",
    TRUE ~ as.character(country)
  )) 

sal <- sal %>% 
  mutate(hour_week = case_when(
    hr_per_week < 40 ~ "<40",
    hr_per_week == 40 ~ "40",
    hr_per_week > 40 ~ ">40"
  )) 

# Drop columns we won't use in our analysis
sal <- select(sal, -c(country, hr_per_week))

```

Now that we have prepped our data, we need to see where we have missing values and where we need to add, or impute, values for those columns. Each dataset will have it's own logical way of imputing missing values, like mean, mode, or median, so take into consideration what you data looks like before filling in your data.

```{r fig.width=8}
# First, we need to convert the '?' data into true missing values
sal[sal=='?'] <- NA

# Refactorize editted columns 
sal$type_employer <- sapply(sal$type_employer, factor) 
sal$country <- sapply(sal$country, factor)
sal$marital <- sapply(sal$marital, factor)
sal$hour_week <- sapply(sal$hour_week, factor)

# Now, we want to visiualize the missing values using the Amelia library
missmap(sal, y.at = c(1), y.labels = c(" "), col=c("red", "black"))
```

Looking at our missingness map, we can see that we have a 1% missing amount of information. We will need to deal with those data points into our dataset so that we are using a complete dataset. In our case, we need to find or drop values for occupation and the type of employer (type_employer).

```{r}
# Since we cannot find the "average" occupation or employer, we are just going to discard those observations all together.

sal <- na.omit(sal) # This will drop any rows with NA data

# Now lets remake our missingness map
missmap(sal, y.at = c(1), y.labels = c(" "), col=c("red", "black"))
```

Now, we are going to dig into some visiualization of our data. Since we are in the exploratory phase of our data, we need to see a few more things of our data to make sure we understand what we are trying to achieve with machine learning.

#### Data Exploration 

```{r}
ggplot(sal, aes(age)) +
  geom_histogram(aes(fill=income), color="black", binwidth = 1) +
  theme_bw()

ggplot(sal, aes(hour_week)) + 
  geom_bar(aes(fill=income), stat="count") + 
  theme_bw()

ggplot(sal, aes(region)) +
  geom_bar(aes(fill=income), color='black') +
  theme_bw()
```

#### Machine Learning with Logistic Regression

To begin, we need to split our data set into two group - a training set and a test set. This is an example of __supervised__ learning where we are feeding our algorithm some data where we know the output. The model will 'learn' this pattern and then when we feed it the test set, we can see how well it performs. What this means is, in the future when we give it new data that hasn't been classified, we will be able to gauge how accurate those predictions will be.

```{r}
# A seed is a 'random' code in how the computer will pick rows to split for the test and train data
set.seed(101)

# This line will take a sample on the column we are trying to predict
sample <- sample.split(sal$income, SplitRatio = 0.7)

# This is our training data
train <- subset(sal, sample==T)

# This is our test data
test <- subset(sal, sample==F)
```


```{r}
# This will be our base model with our training dataset
model <- glm(income ~ ., family = binomial(link = 'logit'), data=train)

#
pander::pander(summary(model))
```

```{r}
test$predicted.income <- predict(model, newdata = test, type = 'response')
pander::pander(table(test$income, test$predicted.income > 0.5))

acc <- (6929+1392)/(6929+487+960+1392)
```

### CONCLUSION



### SOURCES


